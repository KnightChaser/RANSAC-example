{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63985d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"OpenCV version: {cv2.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbbfca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_extractor() -> cv2.Feature2D:\n",
    "    \"\"\"\n",
    "    Prefer SIFT (robust for text/edges), fall back to ORB/AKAZE if SIFT not available\n",
    "    \"\"\"\n",
    "    if hasattr(cv2, 'SIFT_create'):\n",
    "        return cv2.SIFT_create(nfeatures=6000)\n",
    "    if hasattr(cv2, 'AKAZE_create'):\n",
    "        return cv2.AKAZE_create()\n",
    "    return cv2.ORB_create(nfeatures=6000)\n",
    "\n",
    "def is_float_desc(desc: np.ndarray) -> bool:\n",
    "    return desc is not None and desc.dtype == np.float32\n",
    "\n",
    "def imread(path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Read an image from a file.\n",
    "    Raises FileNotFoundError if the image cannot be read.\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    if img is None:\n",
    "        raise FileNotFoundError(f\"Image not found: {path}\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3df04fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We don't know the relative pose between these two images\n",
    "A_PATH = \"./images/left_crop.png\"\n",
    "B_PATH = \"./images/right_full.png\"\n",
    "\n",
    "A = imread(Path(A_PATH))\n",
    "B = imread(Path(B_PATH))\n",
    "\n",
    "def imshow(title: str, bgr: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Show a BGR image using matplotlib (which expects RGB).\n",
    "    \"\"\"\n",
    "    rgb = cv2.cvtColor(bgr, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(rgb)\n",
    "    plt.title(title)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "imshow(\"A\", A)\n",
    "imshow(\"B\", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b04ab5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = make_extractor()\n",
    "\n",
    "def detect(gray: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Detect keypoints and compute descriptors.\n",
    "    \"\"\"\n",
    "    keypoints, desc = extractor.detectAndCompute(gray, None)\n",
    "    if desc is None or len(keypoints) < 10:\n",
    "        raise ValueError(\"Not enough keypoints detected.\")\n",
    "    return keypoints, desc\n",
    "\n",
    "A_gray = cv2.cvtColor(A, cv2.COLOR_BGR2GRAY)\n",
    "B_gray = cv2.cvtColor(B, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "A_kp, A_desc = detect(A_gray)\n",
    "B_kp, B_desc = detect(B_gray)\n",
    "\n",
    "print(f\"A: {len(A_kp)} keypoints, desc shape: {A_desc.shape}, float: {is_float_desc(A_desc)}\")\n",
    "print(f\"B: {len(B_kp)} keypoints, desc shape: {B_desc.shape}, float: {is_float_desc(B_desc)}\")\n",
    "\n",
    "# Visual check!\n",
    "# In the following code, we can draw keypoints of each image\n",
    "A_vis = cv2.drawKeypoints(A, A_kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "B_vis = cv2.drawKeypoints(B, B_kp, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "imshow(\"A keypoints\", A_vis)\n",
    "imshow(\"B keypoints\", B_vis)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_descriptors(q_desc: np.ndarray, s_desc: np.ndarray) -> list[list[cv2.DMatch]]:\n",
    "    \"\"\"\n",
    "    Return list of [m, n] pairs, where m is the best match and n is the second best match,\n",
    "    from knnMatch(k=2), or empty list if no matches found.\n",
    "    \"\"\"\n",
    "    if q_desc is None or s_desc is None:\n",
    "        return []\n",
    "    # Use a brute-force matcher with appropriate norm\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_L2 if is_float_desc(q_desc) else cv2.NORM_HAMMING)\n",
    "    raw_matches = matcher.knnMatch(q_desc, s_desc, k=2)\n",
    "    return [pair for pair in raw_matches if len(pair) == 2]\n",
    "\n",
    "# Example: matches for A -> B (query A, scene B)\n",
    "raw_matches_AB = match_descriptors(A_desc, B_desc)\n",
    "print(f\"Found {len(raw_matches_AB)} raw matches between A and B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a82892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _quad_is_sane(quad: np.ndarray, scene_shape: tuple[int, int, int]) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the projected quadrilateral is sane:\n",
    "    - Not None\n",
    "    - Area > 0 and not too small\n",
    "    - At least 3 points within scene bounds\n",
    "    \"\"\"\n",
    "    if quad is None:\n",
    "        return False\n",
    "    \n",
    "    points = quad.reshape(-1, 2).astype(np.float32)\n",
    "    # Area > 0 and convex:\n",
    "    area = cv2.contourArea(points)\n",
    "    if area < 64:\n",
    "        # Too small area\n",
    "        return False\n",
    "    Hs, Ws = scene_shape[:2]\n",
    "    in_bounds = sum(0 <= x <= Ws and 0 <= y <= Hs for x, y in points)\n",
    "    return in_bounds >= 3  # At least 3 points in bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3efcd4c",
   "metadata": {},
   "source": [
    "### RANSAC homography + diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e538a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_corners(q_img: np.ndarray, H: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Project the corners of the query image using the homography H.\n",
    "    Returns the projected corners as a (4,1,2) array.\n",
    "    \"\"\"\n",
    "    h, w = q_img.shape[:2]\n",
    "    corners = np.float32([[0,0],[w,0],[w,h],[0,h]]).reshape(-1,1,2)\n",
    "    return cv2.perspectiveTransform(corners, H)  # shape (4,1,2)\n",
    "\n",
    "def draw_quad(scene_bgr: np.ndarray, quad: np.ndarray, title: str = \"Projected Quad\") -> None:\n",
    "    \"\"\"\n",
    "    Draw the projected quadrilateral on the scene image.\n",
    "    \"\"\"\n",
    "    vis = scene_bgr.copy()\n",
    "    quad_i = np.int32(quad.reshape(-1,2))\n",
    "    cv2.polylines(vis, [quad_i], True, (0,255,0), 3, cv2.LINE_AA)\n",
    "    imshow(title, vis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff15791a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ransac_localize(q_kps: list[cv2.KeyPoint],\n",
    "                    s_kps: list[cv2.KeyPoint],\n",
    "                    raw_matches: list[list[cv2.DMatch]],\n",
    "                    ratio: float = 0.75,\n",
    "                    ransac_thresh: float = 4.0,\n",
    "                    min_inliers: int = 12) -> dict:\n",
    "    \"\"\"\n",
    "    Perform RANSAC to find a homography between query and scene keypoints.\n",
    "    \"\"\"\n",
    "    # Lowe's ratio\n",
    "    good = [m for m, n in raw_matches if m.distance < ratio * n.distance]\n",
    "    if len(good) < 4:\n",
    "        return dict(ok=False, reason=\"too_few_good_matches\", good=good,\n",
    "                    H=None, mask=None, inliners=0)\n",
    "\n",
    "    src = np.float32([q_kps[m.queryIdx].pt for m in good]).reshape(-1,2) # (N,2)\n",
    "    dst = np.float32([s_kps[m.trainIdx].pt for m in good]).reshape(-1,2) # (N,2)\n",
    "    H, mask = cv2.findHomography(src, dst, cv2.RANSAC, ransacReprojThreshold=ransac_thresh)\n",
    "    if H is None:\n",
    "        # No homography found\n",
    "        return dict(ok=False, reason=\"no_homography\", good=good,\n",
    "                    H=None, mask=None, inliers=0)\n",
    "    inliers = int(mask.ravel().sum())\n",
    "    \n",
    "    if inliers < min_inliers:\n",
    "        # Too few inliers\n",
    "        return dict(ok=False, reason=\"too_few_inliers\", good=good,\n",
    "                    H=H, mask=mask, inliers=inliers)\n",
    "    \n",
    "    return dict(ok=True, reason=\"success\", good=good,\n",
    "                H=H, mask=mask, inliers=inliers)\n",
    "\n",
    "def try_direction(query_bgr: np.ndarray, scene_bgr: np.ndarray,\n",
    "                  query_kp: list[cv2.KeyPoint], query_desc: np.ndarray,\n",
    "                  scene_kp: list[cv2.KeyPoint], scene_desc: np.ndarray,\n",
    "                  tag: str) -> dict:\n",
    "    \"\"\"\n",
    "    Try to localize query in scene using RANSAC.\n",
    "    Returns a dictionary with results.\n",
    "    \"\"\"\n",
    "    raw_matches = match_descriptors(query_desc, scene_desc)\n",
    "    res = ransac_localize(query_kp, scene_kp, raw_matches)\n",
    "    res[\"tag\"] = tag\n",
    "    res[\"quad\"] = project_corners(query_bgr, res[\"H\"]) if res[\"ok\"] else None\n",
    "\n",
    "    # (Optional) also track ratio to compare scenes of different sizes:\n",
    "    res[\"inlier_ratio\"] = (res[\"inliers\"] / max(1, len(res[\"good\"])))\n",
    "    return res\n",
    "\n",
    "def draw_inliers(q_img: np.ndarray,\n",
    "                 s_img: np.ndarray,\n",
    "                 q_kps: list[cv2.KeyPoint],\n",
    "                 s_kps: list[cv2.KeyPoint],\n",
    "                 matches: list[cv2.DMatch],\n",
    "                 mask: np.ndarray,\n",
    "                 title: str = \"Inlier Matches\") -> None:\n",
    "    \"\"\"\n",
    "    Draw inlier matches between query and scene images.\n",
    "    \"\"\"\n",
    "    inlier_matches = [m for m, inlier in zip(matches, mask.ravel()) if inlier]\n",
    "    vis = cv2.drawMatches(q_img, q_kps, s_img, s_kps, inlier_matches, None,\n",
    "                          matchColor=(0,255,0), singlePointColor=(255,0,0),\n",
    "                          flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    imshow(title, vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9be576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _score(res: dict) -> tuple[int, float]:\n",
    "    \"\"\"\n",
    "    Robust scorer: prefer more inliers, tie-break by inlier ratio\n",
    "    \"\"\"\n",
    "    return (res.get(\"inliers\", 0), res.get(\"inlier_ratio\", 0.0))\n",
    "\n",
    "AB = try_direction(A, B, A_kp, A_desc, B_kp, B_desc, \"A inside B\")\n",
    "BA = try_direction(B, A, B_kp, B_desc, A_kp, A_desc, \"B inside A\")\n",
    "\n",
    "candidates = [r for r in (AB, BA) if r.get(\"ok\")]\n",
    "\n",
    "if candidates:\n",
    "    best = max(candidates, key=_score)\n",
    "    print(f\"[✓] Inclusion detected: {best['tag']} | inliers={best['inliers']} ({best['inlier_ratio']:.2f} ratio)\")\n",
    "else:\n",
    "    # graceful failure path\n",
    "    best_failed = max((AB, BA), key=_score)\n",
    "    print(\"No confident inclusion.\")\n",
    "    print(\"Best try:\", best_failed[\"tag\"],\n",
    "          \"| inliers:\", best_failed.get(\"inliers\", 0),\n",
    "          \"| reason:\", best_failed.get(\"reason\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c0f32",
   "metadata": {},
   "source": [
    "### Check the overlay alignment by visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14566804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_into_scene(query_bgr: np.ndarray, \n",
    "                    scene_bgr: np.ndarray, \n",
    "                    H: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Warp the query image into scene coordinates with H (query -> scene).\n",
    "    Returns (warped_query_bgr, mask_bool) in scene size.\n",
    "    \"\"\"\n",
    "    Hs, Ws = scene_bgr.shape[:2]\n",
    "    warped = cv2.warpPerspective(query_bgr, H, (Ws, Hs))\n",
    "    # make a mask by warping a white image the same size as query\n",
    "    qmask = np.ones(query_bgr.shape[:2], dtype=np.uint8) * 255\n",
    "    mask = cv2.warpPerspective(qmask, H, (Ws, Hs)) > 0\n",
    "    return warped, mask\n",
    "\n",
    "\n",
    "def overlay_edges(scene_bgr: np.ndarray, \n",
    "                  warped_bgr: np.ndarray, \n",
    "                  mask: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Draw edges from the warped query (cyan) on top of the scene to highlight boundaries.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(warped_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 80, 160)\n",
    "    out = scene_bgr.copy()\n",
    "    # only draw edges inside the valid warp area\n",
    "    edge_locs = (edges > 0) & mask\n",
    "    out[edge_locs] = (0, 255, 255)  # cyan\n",
    "    return out\n",
    "\n",
    "def diff_heatmap(scene_bgr: np.ndarray, \n",
    "                 warped_bgr: np.ndarray, \n",
    "                 mask: np.ndarray, \n",
    "                 alpha: float = 0.6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Show a heatmap of |scene - warped| inside the mask. Warmer = bigger mismatch.\n",
    "    \"\"\"\n",
    "    diff = cv2.absdiff(scene_bgr, warped_bgr)\n",
    "    gray = cv2.cvtColor(diff, cv2.COLOR_BGR2GRAY)\n",
    "    norm = cv2.normalize(gray, None, 0, 255, cv2.NORM_MINMAX).astype(np.uint8)\n",
    "    heat = cv2.applyColorMap(norm, cv2.COLORMAP_JET)\n",
    "    out = scene_bgr.copy()\n",
    "    out[mask] = cv2.addWeighted(scene_bgr[mask], 1 - alpha, heat[mask], alpha, 0)\n",
    "    return out\n",
    "\n",
    "def show_alignment(query_bgr: np.ndarray, \n",
    "                   scene_bgr: np.ndarray, \n",
    "                   H: np.ndarray, \n",
    "                   prefix: str) -> None:\n",
    "    \"\"\"\n",
    "    Convenience: warp query->scene and show 3 complementary views.\n",
    "    \"\"\"\n",
    "    warped, mask = warp_into_scene(query_bgr, scene_bgr, H)\n",
    "    imshow(f\"{prefix} · warped edges\", overlay_edges(scene_bgr, warped, mask))\n",
    "    imshow(f\"{prefix} · diff heatmap\", diff_heatmap(scene_bgr, warped, mask, alpha=0.55))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eee5be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if candidates:\n",
    "    if AB.get(\"ok\"):\n",
    "        draw_quad(B, project_corners(A, AB[\"H\"]), title=\"A is projected into B\")\n",
    "        show_alignment(A, B, AB[\"H\"], prefix=\"A->B alignment\")\n",
    "    if BA.get(\"ok\"):\n",
    "        draw_quad(A, project_corners(B, BA[\"H\"]), title=\"B is projected into A\")\n",
    "        show_alignment(B, A, BA[\"H\"], prefix=\"B->A alignment\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RANSAC-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
